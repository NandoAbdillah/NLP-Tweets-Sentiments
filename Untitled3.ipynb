{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b55b5127-001d-4986-9fa1-785f2245a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78aac5c0-fd31-4b82-b2cd-708a97d6ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./covid19tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a13710-dc3e-4c52-a598-4adb08fc03a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If I smelled the scent of hand sanitizers toda...</td>\n",
       "      <td>If I smelled the scent of hand sanitizers toda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey @Yankees @YankeesPR and @MLB - wouldn't it...</td>\n",
       "      <td>Hey @Yankees @YankeesPR and @MLB - wouldn't it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@diane3443 @wdunlap @realDonaldTrump Trump nev...</td>\n",
       "      <td>@diane3443 @wdunlap @realDonaldTrump Trump nev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@brookbanktv The one gift #COVID19 has give me...</td>\n",
       "      <td>@brookbanktv The one gift #COVID19 has give me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25 July : Media Bulletin on Novel #CoronaVirus...</td>\n",
       "      <td>25 July : Media Bulletin on Novel #CoronaVirus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  If I smelled the scent of hand sanitizers toda...   \n",
       "1  Hey @Yankees @YankeesPR and @MLB - wouldn't it...   \n",
       "2  @diane3443 @wdunlap @realDonaldTrump Trump nev...   \n",
       "3  @brookbanktv The one gift #COVID19 has give me...   \n",
       "4  25 July : Media Bulletin on Novel #CoronaVirus...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  If I smelled the scent of hand sanitizers toda...  \n",
       "1  Hey @Yankees @YankeesPR and @MLB - wouldn't it...  \n",
       "2  @diane3443 @wdunlap @realDonaldTrump Trump nev...  \n",
       "3  @brookbanktv The one gift #COVID19 has give me...  \n",
       "4  25 July : Media Bulletin on Novel #CoronaVirus...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menghilangkan Links\n",
    "df['clean_text'] = df['text'].apply(lambda s : ' '.join(re.sub(r\"http\\S+\" , \"\", s).split()))\n",
    "df['clean_text'] = df['clean_text'].apply(lambda s : ' '.join(re.sub(r\"www.\\S+\\.com\" , \"\", s).split()))\n",
    "\n",
    "df[['text','clean_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e5bffe-5452-411e-b6bb-ada9f92fbfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Menghilangkan links\n",
    "# df['clean_text'] = df['text'].apply(lambda s : ' '.join(re.sub(r\"http\\S+\" , \"\", s).split()))\n",
    "# df['clean_text'] = df['clean_text'].apply(lambda s : ' '.join(re.sub(r\"www.\\S+\\.com\" , \"\" , s).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89838282-dc8f-448a-87c5-25123bd8f9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If I smelled the scent of hand sanitizers toda...</td>\n",
       "      <td>If I smelled the scent of hand sanitizers toda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey @Yankees @YankeesPR and @MLB - wouldn't it...</td>\n",
       "      <td>Hey Yankees YankeesPR and MLB wouldnt it have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@diane3443 @wdunlap @realDonaldTrump Trump nev...</td>\n",
       "      <td>diane3443 wdunlap realDonaldTrump Trump never ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@brookbanktv The one gift #COVID19 has give me...</td>\n",
       "      <td>brookbanktv The one gift COVID19 has give me i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25 July : Media Bulletin on Novel #CoronaVirus...</td>\n",
       "      <td>25 July Media Bulletin on Novel CoronaVirusUpd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  If I smelled the scent of hand sanitizers toda...   \n",
       "1  Hey @Yankees @YankeesPR and @MLB - wouldn't it...   \n",
       "2  @diane3443 @wdunlap @realDonaldTrump Trump nev...   \n",
       "3  @brookbanktv The one gift #COVID19 has give me...   \n",
       "4  25 July : Media Bulletin on Novel #CoronaVirus...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  If I smelled the scent of hand sanitizers toda...  \n",
       "1  Hey Yankees YankeesPR and MLB wouldnt it have ...  \n",
       "2  diane3443 wdunlap realDonaldTrump Trump never ...  \n",
       "3  brookbanktv The one gift COVID19 has give me i...  \n",
       "4  25 July Media Bulletin on Novel CoronaVirusUpd...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menghapus tanda baca \n",
    "import string\n",
    "df['clean_text'] = df['clean_text'].apply(lambda s : ' '.join(re.sub(f\"[{string.punctuation}]\", \"\", s).split()))\n",
    "df[['text','clean_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398b50d-dde4-46be-a79b-91eb71850779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Menghapus tanda baca \n",
    "# df['clean_text'] = df['clean_text'].apply(lambda s : ' '.join(re.sub(f\"[{string.punctuation}]\", \"\", s).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23909ad-115d-42c3-a411-1af28151712b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If I smelled the scent of hand sanitizers toda...</td>\n",
       "      <td>If I smelled the scent of hand sanitizers toda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey @Yankees @YankeesPR and @MLB - wouldn't it...</td>\n",
       "      <td>Hey Yankees YankeesPR and MLB wouldnt it have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@diane3443 @wdunlap @realDonaldTrump Trump nev...</td>\n",
       "      <td>diane wdunlap realDonaldTrump Trump never once...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@brookbanktv The one gift #COVID19 has give me...</td>\n",
       "      <td>brookbanktv The one gift COVID has give me is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25 July : Media Bulletin on Novel #CoronaVirus...</td>\n",
       "      <td>July Media Bulletin on Novel CoronaVirusUpdat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  If I smelled the scent of hand sanitizers toda...   \n",
       "1  Hey @Yankees @YankeesPR and @MLB - wouldn't it...   \n",
       "2  @diane3443 @wdunlap @realDonaldTrump Trump nev...   \n",
       "3  @brookbanktv The one gift #COVID19 has give me...   \n",
       "4  25 July : Media Bulletin on Novel #CoronaVirus...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  If I smelled the scent of hand sanitizers toda...  \n",
       "1  Hey Yankees YankeesPR and MLB wouldnt it have ...  \n",
       "2  diane wdunlap realDonaldTrump Trump never once...  \n",
       "3  brookbanktv The one gift COVID has give me is ...  \n",
       "4   July Media Bulletin on Novel CoronaVirusUpdat...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menghilangkan angka\n",
    "df['clean_text'].replace('\\d+' , '', regex = True, inplace = True)  \n",
    "df[['text','clean_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f368ea-a58b-4e2a-9602-bbaf6146adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['clean_text'].replace('\\d+' , '', regex = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "973d37ef-a768-4a02-aa87-af4049b706c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If I smelled the scent of hand sanitizers toda...</td>\n",
       "      <td>If I smelled the scent of hand sanitizers toda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey @Yankees @YankeesPR and @MLB - wouldn't it...</td>\n",
       "      <td>Hey Yankees YankeesPR and MLB wouldnt it have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@diane3443 @wdunlap @realDonaldTrump Trump nev...</td>\n",
       "      <td>diane wdunlap realDonaldTrump Trump never once...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@brookbanktv The one gift #COVID19 has give me...</td>\n",
       "      <td>brookbanktv The one gift COVID has give me is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25 July : Media Bulletin on Novel #CoronaVirus...</td>\n",
       "      <td>July Media Bulletin on Novel CoronaVirusUpdat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  If I smelled the scent of hand sanitizers toda...   \n",
       "1  Hey @Yankees @YankeesPR and @MLB - wouldn't it...   \n",
       "2  @diane3443 @wdunlap @realDonaldTrump Trump nev...   \n",
       "3  @brookbanktv The one gift #COVID19 has give me...   \n",
       "4  25 July : Media Bulletin on Novel #CoronaVirus...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  If I smelled the scent of hand sanitizers toda...  \n",
       "1  Hey Yankees YankeesPR and MLB wouldnt it have ...  \n",
       "2  diane wdunlap realDonaldTrump Trump never once...  \n",
       "3  brookbanktv The one gift COVID has give me is ...  \n",
       "4   July Media Bulletin on Novel CoronaVirusUpdat...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Menghilangkan Semua emoji\n",
    "\n",
    "def remove_emojis(text) :\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['clean_text'].apply(remove_emojis)\n",
    "df[['text','clean_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fc856d-608d-41ed-a962-103b16140e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text) :\n",
    "    text = text.encode('ascii' , 'ignore').decode('ascii')\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['clean_text'].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b62c048-bcde-44f8-8d1a-754fedf30d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case folding\n",
    "df['clean_text'] = df['clean_text'].apply(lambda s : s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f19dcf05-f033-43dc-93b8-9dd9c5d1815d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If I smelled the scent of hand sanitizers toda...</td>\n",
       "      <td>if i smelled the scent of hand sanitizers toda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey @Yankees @YankeesPR and @MLB - wouldn't it...</td>\n",
       "      <td>hey yankees yankeespr and mlb wouldnt it have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@diane3443 @wdunlap @realDonaldTrump Trump nev...</td>\n",
       "      <td>diane wdunlap realdonaldtrump trump never once...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@brookbanktv The one gift #COVID19 has give me...</td>\n",
       "      <td>brookbanktv the one gift covid has give me is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25 July : Media Bulletin on Novel #CoronaVirus...</td>\n",
       "      <td>july media bulletin on novel coronavirusupdat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  If I smelled the scent of hand sanitizers toda...   \n",
       "1  Hey @Yankees @YankeesPR and @MLB - wouldn't it...   \n",
       "2  @diane3443 @wdunlap @realDonaldTrump Trump nev...   \n",
       "3  @brookbanktv The one gift #COVID19 has give me...   \n",
       "4  25 July : Media Bulletin on Novel #CoronaVirus...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  if i smelled the scent of hand sanitizers toda...  \n",
       "1  hey yankees yankeespr and mlb wouldnt it have ...  \n",
       "2  diane wdunlap realdonaldtrump trump never once...  \n",
       "3  brookbanktv the one gift covid has give me is ...  \n",
       "4   july media bulletin on novel coronavirusupdat...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text', 'clean_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20d10f4e-3404-44eb-bbdb-d9b8ad8e1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['clean_text'] = dff['clean_text'].apply(lambda s : s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cd6e2d1-888e-4f1d-9fe4-61596d802ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3a63fc9-e786-473d-b318-ef75cb4c71e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghilangkan Stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ce1e025-721d-4679-a253-232f6a2b30b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english')) #Bisa diganti bahasanya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abf27dbd-2c15-4b3d-9c79-b4c77213db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text) :\n",
    "    words = text.split()\n",
    "    noise_free_word = []\n",
    "\n",
    "    for word in words :\n",
    "        if word not in stop :\n",
    "            noise_free_word.append(word)\n",
    "    noise_free_text = ' '.join(noise_free_word)\n",
    "    return noise_free_text\n",
    "\n",
    "\n",
    "df['clean_text'] = df['clean_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36010e5-ee18-4d85-b9b1-97c12ecf858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text) :\n",
    "    words = text.split()\n",
    "    noise_free_words = []\n",
    "\n",
    "    for word in words :\n",
    "        if word not in stop :\n",
    "            noise_free_words.append(word)\n",
    "    noise_free_text = ' '.join(noise_free_words)\n",
    "\n",
    "    return noise_free_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7389ae39-de0c-48e8-b32f-86b273d7316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df['clean_text'] = df['clean_text'].apply(lambda s : tokenizer.tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b74da68-acaf-4c06-82ec-6022a04158d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df['clean_text'] = df['clean_text'].apply(lambda s : tokenizer.tokenize(s))\n",
    "\n",
    "\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# df['clean_text'] = df['clean_text'].apply(lambda s : tokenizer.tokenize(s))\n",
    "\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# df['clean_text'] = df['clean_text'].apply(lambda s : tokenizer.tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a19389d-e0c7-42a0-a45f-9905595e9663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming english\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizing(text) :\n",
    "    lemmatized_word = []\n",
    "    for word in text :\n",
    "        lemmatized_word.append(lemmatizer.lemmatize(word, pos= 'v'))\n",
    "    text = lemmatized_word\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8760fbd-f9d9-4838-9587-8f23a9473913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lemmatizing english \n",
    "# from nltk.stem import WordNetLemmtizer\n",
    "\n",
    "# lemmatizer = WordNetLematizer()\n",
    "# def lemmatinzing (text) :\n",
    "#     lemmatized_word = []\n",
    "#     for word in text :\n",
    "#         lemmatized_word.append(lemmatizer.lemmatize(word, pos = 'v'))\n",
    "#     text = lemmatized_word\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b5cafbc-5c2d-42ae-83fe-c4b95d604060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the_result = preprocess_text(['movies', 'amazing', 'overrated', 'supposed', 'lies','beens', 'taking','while']\n",
    "# exp_result = lemmatizing(['movies', 'amazing', 'supposed', 'lies', 'beens', 'takes', 'ringing' ,'bananas', 'bought', 'went'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a62f5df-1a7e-4f6e-95ec-3f1eea8aac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(exp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98f328ab-4f50-48f0-98fd-001e2097ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def sastrawing(text) :\n",
    "    sastra_word = []\n",
    "    for word in text :\n",
    "        sastra_word.append(stemmer.stem(word))\n",
    "    text = sastra_word\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33ce45f-71d7-446d-930d-8bc7963a0539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizing(text) :\n",
    "    lemmatized_word = []\n",
    "    for word in text :\n",
    "        lemmatized_word.append(lemmatizer.lematize(word, pos='v'))\n",
    "    text = lemmatized_word\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a05524-606f-4f80-a4c3-b6b0f9c385a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# factory = StemmerFactory()\n",
    "# stemmer = factory.create_stemmer()\n",
    "\n",
    "# def sastrawing(text) :\n",
    "#     sastra_word = []\n",
    "#     for word in text :\n",
    "#         sastra_word.append(stemmer.stem(word))\n",
    "#     text = sastra_word\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "844c6fc8-cee9-4f5e-8d8f-043f3f5facdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['clean_text'].apply(lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77978931-3e42-47ff-b962-aa80e50dd2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If I smelled the scent of hand sanitizers toda...</td>\n",
       "      <td>[smell, scent, hand, sanitizers, today, someon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey @Yankees @YankeesPR and @MLB - wouldn't it...</td>\n",
       "      <td>[hey, yankees, yankeespr, mlb, wouldnt, make, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@diane3443 @wdunlap @realDonaldTrump Trump nev...</td>\n",
       "      <td>[diane, wdunlap, realdonaldtrump, trump, never...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@brookbanktv The one gift #COVID19 has give me...</td>\n",
       "      <td>[brookbanktv, one, gift, covid, give, apprecia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25 July : Media Bulletin on Novel #CoronaVirus...</td>\n",
       "      <td>[july, media, bulletin, novel, coronavirusupda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  If I smelled the scent of hand sanitizers toda...   \n",
       "1  Hey @Yankees @YankeesPR and @MLB - wouldn't it...   \n",
       "2  @diane3443 @wdunlap @realDonaldTrump Trump nev...   \n",
       "3  @brookbanktv The one gift #COVID19 has give me...   \n",
       "4  25 July : Media Bulletin on Novel #CoronaVirus...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  [smell, scent, hand, sanitizers, today, someon...  \n",
       "1  [hey, yankees, yankeespr, mlb, wouldnt, make, ...  \n",
       "2  [diane, wdunlap, realdonaldtrump, trump, never...  \n",
       "3  [brookbanktv, one, gift, covid, give, apprecia...  \n",
       "4  [july, media, bulletin, novel, coronavirusupda...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text','clean_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c90c78c-a36e-469c-ba91-5f9beffdaa0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclean_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msastrawing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4762\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[19], line 9\u001b[0m, in \u001b[0;36msastrawing\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      7\u001b[0m sastra_word \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text :\n\u001b[1;32m----> 9\u001b[0m     sastra_word\u001b[38;5;241m.\u001b[39mappend(\u001b[43mstemmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m text \u001b[38;5;241m=\u001b[39m sastra_word\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\Sastrawi\\Stemmer\\CachedStemmer.py:20\u001b[0m, in \u001b[0;36mCachedStemmer.stem\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     18\u001b[0m     stems\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mget(word))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelegatedStemmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mset(word, stem)\n\u001b[0;32m     22\u001b[0m     stems\u001b[38;5;241m.\u001b[39mappend(stem)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\Sastrawi\\Stemmer\\Stemmer.py:27\u001b[0m, in \u001b[0;36mStemmer.stem\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     24\u001b[0m stems \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[1;32m---> 27\u001b[0m     stems\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(stems)\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\Sastrawi\\Stemmer\\Stemmer.py:36\u001b[0m, in \u001b[0;36mStemmer.stem_word\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem_plural_word(word)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem_singular_word\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\Sastrawi\\Stemmer\\Stemmer.py:84\u001b[0m, in \u001b[0;36mStemmer.stem_singular_word\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Stem a singular word to its common stem form.\"\"\"\u001b[39;00m\n\u001b[0;32m     83\u001b[0m context \u001b[38;5;241m=\u001b[39m Context(word, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisitor_provider)\n\u001b[1;32m---> 84\u001b[0m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context\u001b[38;5;241m.\u001b[39mresult\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Context.py:37\u001b[0m, in \u001b[0;36mContext.execute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute stemming process; the result can be retrieved with result\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#step 1 - 5\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_stemming_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#step 6\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_word):\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Context.py:80\u001b[0m, in \u001b[0;36mContext.start_stemming_process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m#step 4, 5\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_prefixes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_word):\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Context.py:89\u001b[0m, in \u001b[0;36mContext.remove_prefixes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_prefixes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m---> 89\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccept_prefix_visitors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix_pisitors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_word):\n\u001b[0;32m     91\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Context.py:110\u001b[0m, in \u001b[0;36mContext.accept_prefix_visitors\u001b[1;34m(self, visitors)\u001b[0m\n\u001b[0;32m    108\u001b[0m removalCount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremovals)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m visitor \u001b[38;5;129;01min\u001b[39;00m visitors:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccept\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisitor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary\u001b[38;5;241m.\u001b[39mcontains(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_word):\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_word\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Context.py:97\u001b[0m, in \u001b[0;36mContext.accept\u001b[1;34m(self, visitor)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccept\u001b[39m(\u001b[38;5;28mself\u001b[39m, visitor):\n\u001b[1;32m---> 97\u001b[0m     \u001b[43mvisitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\Sastrawi\\Stemmer\\Context\\Visitor\\AbstractDisambiguatePrefixRule.py:14\u001b[0m, in \u001b[0;36mAbstractDisambiguatePrefixRule.visit\u001b[1;34m(self, context)\u001b[0m\n\u001b[0;32m     11\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m disambiguator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisambiguators:\n\u001b[1;32m---> 14\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdisambiguator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisambiguate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_word\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mdictionary\u001b[38;5;241m.\u001b[39mcontains(result):\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\Lib\\site-packages\\Sastrawi\\Morphology\\Disambiguator\\DisambiguatorPrefixRule28.py:21\u001b[0m, in \u001b[0;36mDisambiguatorPrefixRule28b.disambiguate\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDisambiguatorPrefixRule28b\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Disambiguate Prefix Rule 28b\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    Rule 28b : pen{V} -> pe-t{V}\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdisambiguate\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Disambiguate Prefix Rule 28b\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m        Rule 28b : pen{V} -> pe-t{V}\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m         matches \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^pen([aiueo])(.*)$\u001b[39m\u001b[38;5;124m'\u001b[39m, word)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['clean_text'] = df['clean_text'].apply(sastrawing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b614383-e690-46df-8416-afac4d9d06f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aku anggap kamu adalah orang yang selalu buat ceria walaupun kadang aku benci'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str = 'aku menganggap kamu adalah seorang yang selalu membuatku ceria walaupun kadang aku membencimu'\n",
    "stemmer.stem(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915e330e-7e3d-4ee5-8d53-0aad3e993031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
